{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PlISOzAVvzX"
      },
      "source": [
        "# Linear & Polynomial Regression (Analytic)\n",
        "\n",
        "We will explore **linear regression** and **polynomial regression** using a synthetic dataset (`synthetic_regression.csv` with columns `x, y`). All solutions must use **analytic (closed-form)** formulas — **no gradient descent, no library `.fit()`** methods. Implement everything directly in **NumPy**.\n",
        "\n",
        "## Tasks\n",
        "1. **70/30 Train–Test Split (Unregularized)**\n",
        "   - Split the data into 70% train / 30% test (random, reproducible).\n",
        "\n",
        "   - Fit the following models:\n",
        "     - Linear regression (polynomial degree 1),\n",
        "     - Polynomial regression with degrees $k\\in \\{2, 5, 10, 15\\}$.\n",
        "\n",
        "   - For each model, build the design matrix explicitly:  For each datapoint, the row is given by\n",
        "\n",
        "     $$\\Phi(x) = [1, x, x^2, \\\\dots, x^k].$$\n",
        "\n",
        "     (let's call the design matrix $\\Phi$ instead of $X$.)\n",
        "\n",
        "   - Solve using the equations we derived in class:\n",
        "\n",
        "     $$\\mathbf{\\theta}^* = (\\Phi^\\top\\Phi)^{-1}\\Phi^\\top \\mathbf{y}.$$\n",
        "\n",
        "   - Compute **training error** and **test error**.\n",
        "\n",
        "   - Plot (a) the dataset points with all model fits on one figure, and (b) a **bar chart** of training vs test errors.\n",
        "\n",
        "2. **10-Fold Cross-Validation (Unregularized)**\n",
        "   - Implement 10-fold CV yourself (shuffle indices once, split into folds).\n",
        "\n",
        "   - For each degree {1, 2, 5, 10, 15}, compute the **average test error** across folds.\n",
        "\n",
        "   - Plot a **bar chart** comparing the average test error across all models. Conclude the best hypothesis class.\n",
        "\n",
        "3. **Repeat (1) and (2) with Ridge Regularization**.\n",
        "   - Use ridge regression with: $$\\mathbf{\\theta}^*_\\lambda = (\\Phi^\\top\\Phi + \\lambda I)^{-1}\\Phi^\\top \\mathbf{y}.$$\n",
        "\n",
        "   - **Take $\\lambda = 1$** (fixed).\n",
        "\n",
        "   - Show the same plots: fitted curves, bar chart of train/test errors, and bar chart of 10-fold average test errors.\n",
        "\n",
        "### Notes\n",
        "- If any bar chart scale makes some bars invisible, **use a logarithmic y-axis**: `plt.yscale(\"log\")`.\n",
        "- Keep your code structured and use the provided skeleton below.\n",
        "\n",
        "\n",
        "\n",
        "## Functions to Implement\n",
        "\n",
        "For this assignment, you will **write the following functions yourself**.  \n",
        "Each function connects the **mathematical definition** we studied in class to working **NumPy code**.  \n",
        "\n",
        "---\n",
        "\n",
        "## 1. `design_matrix_poly_1d(x_column, degree, include_bias=True)`\n",
        "- Build the design matrix **Φ** for **polynomial regression**:  \n",
        "  \\[\n",
        "  \\Phi(x) = [1, x, x^2, \\dots, x^d]\n",
        "  \\]\n",
        "- **Input:** vector of \\(x\\)-values (shape \\(n\\)), degree \\(d\\).  \n",
        "- **Output:** design matrix of shape \\(n \\times (d+1)\\).  \n",
        "\n",
        "---\n",
        "\n",
        "## 2. `normal_equation(Phi, y)`\n",
        "- Compute the **closed-form OLS solution**:  \n",
        "  \\[\n",
        "  \\hat{\\theta} = (\\Phi^\\top \\Phi)^{-1} \\Phi^\\top y\n",
        "  \\]\n",
        "  (or use the pseudoinverse if singular).  \n",
        "- **Input:** design matrix Φ, targets \\(y\\).  \n",
        "- **Output:** regression coefficient vector \\(\\theta\\).  \n",
        "\n",
        "---\n",
        "\n",
        "### 3. `ridge_closed_form(Phi, y, lam)`\n",
        "- Compute the **ridge regression** solution:  \n",
        "  \\[\n",
        "  \\hat{\\theta}_\\lambda = (\\Phi^\\top \\Phi + \\lambda I)^{-1} \\Phi^\\top y\n",
        "  \\]\n",
        "- **Input:** design matrix Φ, targets \\(y\\), regularization parameter \\(\\lambda\\).  \n",
        "- **Output:** coefficient vector \\(\\theta\\).  \n",
        "\n",
        "---\n",
        "\n",
        "### 4. `predict(Phi, theta)`\n",
        "- Generate predictions:  \n",
        "  \\[\n",
        "  \\hat{y} = \\Phi \\theta\n",
        "  \\]\n",
        "- **Input:** design matrix Φ, coefficients \\(\\theta\\).  \n",
        "- **Output:** predicted values.  \n",
        "\n",
        "---\n",
        "\n",
        "### 5. `err(y_true, y_pred)`\n",
        "- Compute the **average squared error**:  \n",
        "  \\[\n",
        "  \\text{Error} = \\frac{1}{n} \\sum_i (y_i - \\hat{y}_i)^2\n",
        "  \\]\n",
        "- **Input:** true values \\(y\\), predicted values \\(\\hat{y}\\).  \n",
        "- **Output:** scalar error.  \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Functions to Use: The following functions are given. Feel free to use them.\n",
        "\n",
        "### 6. `kfold_indices(n, K, seed=0)`\n",
        "- Generate index splits for **K-fold cross-validation**.  \n",
        "- **Input:** dataset size \\(n\\), number of folds \\(K\\), optional random seed.  \n",
        "- **Output:** list of \\((\\text{train\\_idx}, \\text{val\\_idx})\\) pairs.  \n",
        "\n",
        "---\n",
        "\n",
        "### 7. `train_test_split_indices(n, test_ratio=0.3, seed=42)`\n",
        "- Randomly split dataset into **training** and **test** sets.  \n",
        "- **Input:** dataset size \\(n\\), test ratio, optional random seed.  \n",
        "- **Output:** two arrays: `train_idx`, `test_idx`.  \n",
        "\n",
        "---\n",
        "\n",
        "### 8. `load_csv_xy(path)`\n",
        "- Load a CSV file with columns **`x`** and **`y`**.  \n",
        "- **Output:**  \n",
        "  - \\(X\\): array of shape \\(n \\times 1\\),  \n",
        "  - \\(y\\): vector of shape \\(n\\).  \n",
        "\n",
        "---\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n"
      ],
      "id": "-PlISOzAVvzX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zSsvl1MVvzZ"
      },
      "source": [
        "## Starter Skeleton (fill the TODOs)\n",
        "Update the CSV path to where you saved `synthetic_regression.csv`.\n"
      ],
      "id": "8zSsvl1MVvzZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmmce0_uVvzZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def design_matrix_poly_1d(x_column: np.ndarray, degree: int, include_bias: bool=True) -> np.ndarray:\n",
        "    \"\"\"Return Vandermonde-style design matrix [1, x, x^2, ..., x^degree].\"\"\"\n",
        "    YOUR CODE\n",
        "\n",
        "\n",
        "def normal_equation(Phi: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Closed-form least squares: theta = (Phi^T Phi)^{-1} Phi^T y.\"\"\"\n",
        "\n",
        "\n",
        "def ridge_closed_form(Phi: np.ndarray, y: np.ndarray, lam: float) -> np.ndarray:\n",
        "    \"\"\"Closed-form ridge: theta = (Phi^T Phi + λI)^{-1} Phi^T y.\"\"\"\n",
        "\n",
        "\n",
        "def predict(Phi: np.ndarray, theta: np.ndarray) -> np.ndarray:\n",
        "\n",
        "\n",
        "def err(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "\n",
        "\n",
        "def kfold_indices(n: int, K: int, seed: int = 0):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = np.arange(n)\n",
        "    rng.shuffle(idx)\n",
        "    folds = np.array_split(idx, K)\n",
        "    splits = []\n",
        "    for k in range(K):\n",
        "        val_idx = folds[k]\n",
        "        train_idx = np.concatenate([folds[i] for i in range(K) if i != k])\n",
        "        splits.append((train_idx, val_idx))\n",
        "    return splits\n",
        "\n",
        "def train_test_split_indices(n: int, test_ratio: float = 0.3, seed: int = 42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = np.arange(n)\n",
        "    rng.shuffle(idx)\n",
        "    n_test = int(round(test_ratio * n))\n",
        "    test_idx = idx[:n_test]\n",
        "    train_idx = idx[n_test:]\n",
        "    return train_idx, test_idx\n",
        "\n",
        "def load_csv_xy(path: str):\n",
        "    xs, ys = [], []\n",
        "    with open(path, \"r\") as f:\n",
        "        rd = csv.DictReader(f)\n",
        "        for row in rd:\n",
        "            xs.append(float(row[\"x\"]))\n",
        "            ys.append(float(row[\"y\"]))\n",
        "    X = np.array(xs).reshape(-1, 1)\n",
        "    y = np.array(ys)\n",
        "    return X, y\n"
      ],
      "id": "jmmce0_uVvzZ"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gRmB_BkcfTKa"
      },
      "id": "gRmB_BkcfTKa",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
